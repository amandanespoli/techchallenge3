{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "mount_file_id": "1jGySviHAc3WL-Rp9zServnP89a2H4E7r",
      "authorship_tag": "ABX9TyNWiIkim/XCv7ZyNK3LMoYN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amandanespoli/techchallenge3/blob/main/Fine_Tuning_Tech_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparação do ambiente e importação de bibliotecas\n"
      ],
      "metadata": {
        "id": "HlKelzoJ64HL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Slv8R6IoH1Sf",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bs4\n",
        "!pip install tqdm\n",
        "!pip install lxml"
      ],
      "metadata": {
        "id": "5j-YaN0T7GbG",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth"
      ],
      "metadata": {
        "id": "ftQRtyHxDotE",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade --no-deps 'unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git'"
      ],
      "metadata": {
        "id": "5hU11XM7D6q7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-deps torch xformers trl peft accelerate bitsandbytes triton"
      ],
      "metadata": {
        "id": "5mTO0okAFjnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install weave"
      ],
      "metadata": {
        "id": "BCtDGPmjvyco"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arN4ou3Q6QEL"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "import os\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "import torch\n",
        "from typing import List, Dict, Any\n",
        "import re\n",
        "from datasets import load_dataset\n",
        "import weave\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tratamendo de Dataset - Gera arquivo JSON"
      ],
      "metadata": {
        "id": "pgzpj6wA9-pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizacao do Dataset MedQuaAD, originado do repositorio github abaixo"
      ],
      "metadata": {
        "id": "zwGZO4b1VW0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/abachaa/MedQuAD"
      ],
      "metadata": {
        "id": "6JvpGjoF_Q5o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A pasta do projeto estará em /content/MedQuAD/\n",
        "import os\n",
        "print(os.listdir('/content/MedQuAD/'))"
      ],
      "metadata": {
        "id": "qLIHJcBz_dre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extracao do coteudo xml de cada arquivo de cada diretorio para criacao de um unico arquivo jsonl no seguinte formato\n",
        "* \"id_origem\": os.path.basename(caminho_arquivo),\n",
        "* \"url_origem\": doc_url,\n",
        "* \"texto_llm\": formato_llm\n",
        "\n",
        "Sendo o conteudo do formato_llm o seguinte\n",
        "* f\"Responda à seguinte pergunta de forma clara e objetiva:\\n[|Pergunta|]: {pergunta}[|ePergunta|]\\n\\n[|Resposta|]: {resposta}[|eResposta|]\"\n",
        "\n",
        "Arquivo de saida ja com limpeza e normalizacao textual ---> Remocao de espacos extras, normalizacao de quebras de linhas e padronizacao de texto\n",
        "\n"
      ],
      "metadata": {
        "id": "mBx93R4RWDCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIRETORIO_XML = [\n",
        "    'MedQuAD/1_CancerGov_QA/',\n",
        "    'MedQuAD/2_GARD_QA/',\n",
        "    'MedQuAD/3_GHR_QA/',\n",
        "    'MedQuAD/4_MPlus_Health_Topics_QA/',\n",
        "    'MedQuAD/5_NIDDK_QA/',\n",
        "    'MedQuAD/6_NINDS_QA/',\n",
        "    'MedQuAD/7_SeniorHealth_QA/',\n",
        "    'MedQuAD/8_NHLBI_QA_XML',\n",
        "    'MedQuAD/9_CDC_QA',\n",
        "    'MedQuAD/10_MPlus_ADAM_QA',\n",
        "    'MedQuAD/11_MPlusDrugs_QA',\n",
        "    'MedQuAD/12_MPlusHerbsSupplements_QA'\n",
        "]\n",
        "\n",
        "\n",
        "dados_para_fine_tuning = []\n",
        "\n",
        "caminhos_arquivos_xml = []\n",
        "\n",
        "for diretorio in DIRETORIO_XML:\n",
        "    caminhos_encontrados = glob.glob(os.path.join(diretorio, '*.xml'))\n",
        "    caminhos_arquivos_xml.extend(caminhos_encontrados)\n",
        "\n",
        "print(f'Número de arquivos XML encontrados: {len(caminhos_arquivos_xml)}')\n",
        "\n",
        "for caminho_arquivo in caminhos_arquivos_xml:\n",
        "    try:\n",
        "        with open(caminho_arquivo, 'r', encoding='utf-8') as f:\n",
        "            conteudo_xml = f.read()\n",
        "\n",
        "        soup = BeautifulSoup(conteudo_xml, 'lxml-xml')\n",
        "\n",
        "        # pegar URL do <Document> uma vez por arquivo (atributo \"url\" presente no seu XML)\n",
        "        doc_tag = soup.find('Document')\n",
        "        doc_url = \"\"\n",
        "        if doc_tag:\n",
        "            doc_url = doc_tag.get('url') or doc_tag.get('docid') or doc_tag.get('id') or \"\"\n",
        "            # se atributo não existir, tentar texto interno que contenha http\n",
        "            if not doc_url:\n",
        "                possible = doc_tag.find(lambda t: t.string and 'http' in t.string)\n",
        "                if possible:\n",
        "                    doc_url = possible.get_text(strip=True)\n",
        "\n",
        "        # coletar todos os QAPair individuais (mais robusto que buscar apenas QAPairs container)\n",
        "        qapairs = soup.find_all('QAPair')\n",
        "        if not qapairs:\n",
        "            # se não houver QAPair individuais, tentar extrair de containers <QAPairs>\n",
        "            containers = soup.find_all('QAPairs')\n",
        "            for c in containers:\n",
        "                qapairs.extend(c.find_all('QAPair'))\n",
        "\n",
        "        # fallback: se ainda vazio, parear <Question> com o próximo <Answer>\n",
        "        if not qapairs:\n",
        "            questions = soup.find_all('Question')\n",
        "            if not questions:\n",
        "                print(f\"[DEBUG] arquivo {os.path.basename(caminho_arquivo)} sem tag <QAPair> ou <Question>, tags topo: {[t.name for t in soup.find_all()[:10]]}\")\n",
        "                continue\n",
        "            for qtag in questions:\n",
        "                atag = qtag.find_next(lambda t: t.name and t.name.lower() == 'answer')\n",
        "                pergunta = ' '.join(qtag.get_text(strip=True).split())\n",
        "                resposta = ' '.join(atag.get_text(strip=True).split()) if atag else \"\"\n",
        "                if pergunta and resposta:\n",
        "                    #formato_llm = f\"Pergunta: {pergunta}\\nResposta: {resposta}\\n\"\n",
        "                    formato_llm = f\"Responda à seguinte pergunta de forma clara e objetiva:\\n[|Pergunta|]: {pergunta}[|ePergunta|]\\n\\n[|Resposta|]: {resposta}[|eResposta|]\"\n",
        "                    dados_para_fine_tuning.append({\n",
        "                        \"id_origem\": os.path.basename(caminho_arquivo),\n",
        "                        \"url_origem\": doc_url,\n",
        "                        \"texto_llm\": formato_llm\n",
        "                    })\n",
        "            print(f\" - Processado com sucesso (fallback): {os.path.basename(caminho_arquivo)}\")\n",
        "            continue\n",
        "\n",
        "\n",
        "        for registro in qapairs:\n",
        "\n",
        "            pergunta_tag = registro.find('Question')\n",
        "            resposta_tags = registro.find_all('Answer')\n",
        "\n",
        "            pergunta = pergunta_tag.get_text(strip=True) if pergunta_tag else \"\"\n",
        "            resposta = ' '.join(' '.join(a.get_text(strip=True).split()) for a in resposta_tags) if resposta_tags else \"\"\n",
        "\n",
        "            pergunta = ' '.join(pergunta.split())\n",
        "            resposta = ' '.join(resposta.split())\n",
        "\n",
        "\n",
        "            if pergunta and resposta:\n",
        "\n",
        "                #formato_llm = f\"Pergunta: {pergunta}\\nResposta: {resposta}\\n\"\n",
        "                formato_llm = f\"Responda à seguinte pergunta de forma clara e objetiva:\\n[|Pergunta|]: {pergunta}[|ePergunta|]\\n\\n[|Resposta|]: {resposta}[|eResposta|]\"\n",
        "\n",
        "                dados_para_fine_tuning.append({\n",
        "                    \"id_origem\": os.path.basename(caminho_arquivo),\n",
        "                    \"url_origem\": doc_url,\n",
        "                    \"texto_llm\": formato_llm\n",
        "                })\n",
        "\n",
        "\n",
        "        print(f\" - Processado com sucesso: {os.path.basename(caminho_arquivo)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" - Erro ao processar arquivo {os.path.basename(caminho_arquivo)}: {e}\")\n",
        "\n",
        "\n",
        "print(\"\\n-- Dados para fine-tuning extraídos --\")\n",
        "print(f\"Total pares extraídos: {len(dados_para_fine_tuning)}\")\n",
        "for dado in dados_para_fine_tuning[:5]:  # Exibir apenas os primeiros\n",
        "    print(f\"Origem: {dado['id_origem']}\")\n",
        "    print(f\"URL: {dado.get('url_origem','')}\")\n",
        "    print(dado['texto_llm'])\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "\n",
        "arquivo_saida = '/content/drive/MyDrive/Projeto Tech Challenge - Fase 3/dados_fine_tuning_medquad.jsonl'\n",
        "print(f\"\\nEscrevendo {len(dados_para_fine_tuning)} pares Q&A no arquivo {arquivo_saida}...\")\n",
        "\n",
        "try:\n",
        "    with open(arquivo_saida, 'w', encoding='utf-8') as outfile:\n",
        "        for entry in tqdm(dados_para_fine_tuning, desc=\"Escrevendo JSONL\"):\n",
        "            # json.dumps() converte o dicionário em uma string JSON em uma única linha\n",
        "            json_line = json.dumps(entry, ensure_ascii=False)\n",
        "            outfile.write(json_line + '\\n')\n",
        "\n",
        "    print(f\"\\nProcessamento concluído. Dataset pronto em: {arquivo_saida}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"ERRO ao escrever JSONL: {e}\")\n"
      ],
      "metadata": {
        "id": "LWvuLM0l-Oe0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatação de JSON gerado na seção anterior"
      ],
      "metadata": {
        "id": "amYUPJxQF-LJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considerando que o arquivo de saida anterior tem o seguinte formato\n",
        "* f\"Responda à seguinte pergunta de forma clara e objetiva:\\n[|Pergunta|]: {pergunta}[|ePergunta|]\\n\\n[|Resposta|]: {resposta}[|eResposta|]\"\n",
        "\n",
        "Foi necessario adaptar abaixo para que as intrucoes, perguntas e repostas ficassem separas em campos diferentes\n",
        "* final_structure: Dict[str, List[str]] = {\n",
        "    \"instruction\": [],\n",
        "\n",
        "    \"input\": [],\n",
        "\n",
        "    \"output\": []\n",
        "}\n",
        "\n",
        "Realizada normalizacao dos espacos e insercao no arquivo apenas se identificada existencia do par pergunta e reposta.\n"
      ],
      "metadata": {
        "id": "o4Nr-9O_cJJv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Any\n",
        "import json\n",
        "import re\n",
        "\n",
        "# --- Nomes dos Arquivos ---\n",
        "INPUT_FILE = \"/content/drive/MyDrive/Projeto Tech Challenge - Fase 3/dados_fine_tuning_medquad.jsonl\"\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/Projeto Tech Challenge - Fase 3/dados_analise_estrutura.json\" # Mudança para .json (Não é JSONL)\n",
        "\n",
        "# --- Definição dos Marcadores e Instrução Estática ---\n",
        "STATIC_INSTRUCTION = \"Responda à seguinte pergunta de forma clara e objetiva:\"\n",
        "# marcadores legacy (caso alguma entrada use esse formato)\n",
        "PROMPT_START_MARKER = \"Pergunta: \"\n",
        "PROMPT_END_MARKER = \"\\nResposta:\"\n",
        "# marcadores usados no seu formato gerado anteriormente\n",
        "MARK_Q_START = r\"\\[\\|Pergunta\\|\\]:\"\n",
        "MARK_Q_END = r\"\\[\\|ePergunta\\|\\]\"\n",
        "MARK_A_START = r\"\\[\\|Resposta\\|\\]:\"\n",
        "MARK_A_END = r\"\\[\\|eResposta\\|\\]\"\n",
        "\n",
        "# --- Variáveis para Estrutura Final de Listas ---\n",
        "final_structure: Dict[str, List[str]] = {\n",
        "    \"instruction\": [],\n",
        "    \"input\": [],\n",
        "    \"output\": []\n",
        "}\n",
        "\n",
        "print(f\"Processando arquivo: {INPUT_FILE}\")\n",
        "\n",
        "try:\n",
        "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            if not line.strip():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                data = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                print(\"Erro ao decodificar JSON na linha (ignorado).\")\n",
        "                continue\n",
        "\n",
        "            # 1) obter o texto que contém pergunta/resposta (suporta chaves diferentes)\n",
        "            texto = \"\"\n",
        "            if isinstance(data, dict):\n",
        "                if \"texto_llm\" in data and isinstance(data[\"texto_llm\"], str):\n",
        "                    texto = data[\"texto_llm\"]\n",
        "                else:\n",
        "                    # tentativas alternativas se dataset vier em formato prompt/completion\n",
        "                    prompt = data.get(\"prompt\", \"\")\n",
        "                    completion = data.get(\"completion\", \"\")\n",
        "                    if prompt or completion:\n",
        "                        texto = prompt + \"\\n\" + completion\n",
        "                    else:\n",
        "                        # algumas versões podem usar 'prompt'/'response' ou 'question'/'answer'\n",
        "                        texto = data.get(\"texto\", \"\") or data.get(\"question\", \"\") + \"\\n\" + data.get(\"answer\", \"\")\n",
        "\n",
        "            if not texto:\n",
        "                continue\n",
        "\n",
        "            pergunta = \"\"\n",
        "            resposta = \"\"\n",
        "\n",
        "            # 2) tentar extrair pelo padrão [|Pergunta|] ... [|ePergunta|] e [|Resposta|] ... [|eResposta|]\n",
        "            q_match = re.search(MARK_Q_START + r\"\\s*(.*?)\\s*\" + MARK_Q_END, texto, flags=re.DOTALL)\n",
        "            a_match = re.search(MARK_A_START + r\"\\s*(.*?)\\s*\" + MARK_A_END, texto, flags=re.DOTALL)\n",
        "\n",
        "            if q_match and a_match:\n",
        "                pergunta = ' '.join(q_match.group(1).split())\n",
        "                resposta = ' '.join(a_match.group(1).split())\n",
        "            else:\n",
        "                # 3) fallback para formato \"Pergunta: ...\\nResposta: ...\"\n",
        "                start_index = texto.find(PROMPT_START_MARKER)\n",
        "                end_index = texto.find(PROMPT_END_MARKER)\n",
        "                if start_index != -1 and end_index != -1 and end_index > start_index:\n",
        "                    pergunta = texto[start_index + len(PROMPT_START_MARKER): end_index].strip()\n",
        "                    # resposta: tudo após PROMPT_END_MARKER\n",
        "                    resposta = texto[end_index + len(PROMPT_END_MARKER):].strip()\n",
        "                else:\n",
        "                    # 4) último fallback: tentar separar por duas linhas (primeira linha = pergunta, restante = resposta)\n",
        "                    parts = [p for p in texto.splitlines() if p.strip()]\n",
        "                    if len(parts) >= 2:\n",
        "                        pergunta = parts[0].strip()\n",
        "                        resposta = \" \".join(p.strip() for p in parts[1:]).strip()\n",
        "\n",
        "            # normaliza espaços\n",
        "            pergunta = ' '.join(pergunta.split()) if pergunta else \"\"\n",
        "            resposta = ' '.join(resposta.split()) if resposta else \"\"\n",
        "\n",
        "            # adicionar se ambos existirem\n",
        "            if pergunta and resposta:\n",
        "                final_structure[\"instruction\"].append(STATIC_INSTRUCTION)\n",
        "                final_structure[\"input\"].append(f\"question:{pergunta}\")\n",
        "                final_structure[\"output\"].append(f\"answer:{resposta}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"Erro: O arquivo de entrada '{INPUT_FILE}' não foi encontrado.\")\n",
        "\n",
        "print(f\"\\nTotal de pares extraídos e formatados: {len(final_structure['instruction'])}\")\n",
        "\n",
        "# --- 4. Geração do Único Arquivo JSON ---\n",
        "if final_structure[\"instruction\"]:\n",
        "    try:\n",
        "        with open(OUTPUT_FILE, 'w', encoding='utf-8') as f:\n",
        "            # Usa json.dump para escrever o objeto completo em uma única estrutura\n",
        "            json.dump(final_structure, f, indent=4, ensure_ascii=False)\n",
        "\n",
        "        print(f\"✅ Arquivo de saída gerado com sucesso: '{OUTPUT_FILE}'\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao escrever o arquivo de saída: {e}\")\n",
        "else:\n",
        "    print(\"❌ Nenhuma linha válida foi processada. O arquivo de saída não foi gerado.\")"
      ],
      "metadata": {
        "id": "uyjuIVhAG8iF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento do modelo com entrada do JSON formatado"
      ],
      "metadata": {
        "id": "1i-jguMQa0zW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparacao do modelo para fine-tuning eficiente com\n",
        "* Ativacao da quantizacao em 4 bits para reducao no uso de memoria e carregamento do modelo llm LLaMA 3 8B com otimizacao do Unsloth."
      ],
      "metadata": {
        "id": "Fy00nGmtftzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_seq_length = 2048\n",
        "dtype = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
        "load_in_4bit = True\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",\n",
        "]\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/llama-3-8b-bnb-4bit\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n"
      ],
      "metadata": {
        "id": "feiIgkRebAVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicacao PEFT usando LoRA no modelo 4 bits carregado anteriormente.\n",
        "\n",
        "Com o objetivo de reduzir o custo de treino, os pesos originais do modelo nao sao treinados e pequenas matrizes LoRA sao adiconadas."
      ],
      "metadata": {
        "id": "aruVoi-bg-Wn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 3407,\n",
        "    use_rslora = False,\n",
        "    loftq_config = None,\n",
        ")"
      ],
      "metadata": {
        "id": "_XKIjomFb6h5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Definicao do template instrucional que define o que eh instrucao, entrada e saida esperada.\n",
        "\n",
        "* Utilizacao do Token de fim de sequencia (EOS) que indica o fim da resposta.\n",
        "\n",
        "* Definicao do texto final de treino com criacao da coluna text, dataset preparado para tokenizacao e fine-tuning"
      ],
      "metadata": {
        "id": "rR6m5qrMjm3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context\n",
        "\n",
        "### Instruction:\n",
        "{}\n",
        "\n",
        "### Input:\n",
        "{}\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "def formatting_prompts_func(examples):\n",
        "  instructions = examples[\"instruction\"]\n",
        "  inputs       = examples[\"input\"]\n",
        "  outputs      = examples[\"output\"]\n",
        "  texts = []\n",
        "  for instruction, input, output in zip(instructions, inputs, outputs):\n",
        "\n",
        "    text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
        "    texts.append(text)\n",
        "  return {\"text\": texts, }\n",
        "pass\n",
        "\n",
        "OUTPUT_FILE = \"/content/drive/MyDrive/Projeto Tech Challenge - Fase 3/dados_analise_estrutura.json\"\n",
        "\n",
        "dataset = load_dataset(\"json\", data_files=OUTPUT_FILE, split=\"train\")\n",
        "dataset = dataset.map(formatting_prompts_func, batched=True,)\n"
      ],
      "metadata": {
        "id": "l2soK6eVeMAO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuracao do treinador de fine-tuning supervisonado para o LLM quantizado e adaptado com LoRA, definindo parametros eficientes para treinar um modelo grande de forma eficiente."
      ],
      "metadata": {
        "id": "j5F5gLrZlZIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False,\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "    ),\n",
        ")"
      ],
      "metadata": {
        "id": "CvBtBHEchW7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gatilho que inicia efetivamente o fine-tuning do modelo e retorna estatisticas do treinamento."
      ],
      "metadata": {
        "id": "JkmssX_YoAmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_stats = trainer.train()"
      ],
      "metadata": {
        "id": "Fsr7kFxAnsKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparacao do modelo para inferencia com a construcao de um prompt no formato Alpaca."
      ],
      "metadata": {
        "id": "AeyLStHtosU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Responda à seguinte pergunta de forma clara e objetiva:\",\n",
        "        \"What is the relationship between Noonan syndrome and polycystic renal disease?\",\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "outputs = model.generate(**inputs, max_new_tokens = 64, use_cache = True)\n",
        "tokenizer.batch_decode(outputs)"
      ],
      "metadata": {
        "id": "9gdsxA9FpXeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mesmo comando de inferencia, mas com utilizacao do TextStreamer que permite visualizar o texto a medido que o modelo o produz."
      ],
      "metadata": {
        "id": "ZnW3teh3pqwP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "inputs = tokenizer(\n",
        "[\n",
        "    alpaca_prompt.format(\n",
        "        \"Responda à seguinte pergunta de forma clara e objetiva:\",\n",
        "        \"What is the relationship between Noonan syndrome and polycystic renal disease?\",\n",
        "        \"\",\n",
        "    )\n",
        "], return_tensors = \"pt\").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer)\n",
        "_ = model.generate(**inputs, streamer = text_streamer, max_new_tokens = 128)\n"
      ],
      "metadata": {
        "id": "wDSZ-SGmq_Vt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Salvamento do modelo LoRA treinado e seu tokenizer."
      ],
      "metadata": {
        "id": "qay45mt6qK2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/Projeto Tech Challenge - Fase 3/lora_model\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/Projeto Tech Challenge - Fase 3/lora_model\")\n"
      ],
      "metadata": {
        "id": "eUaSoTb_sT_Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Login no Hugging face"
      ],
      "metadata": {
        "id": "clI1O7lfqXU3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "KuRsTudHEMsz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Exportacao do modelo treinado no formato GGUF, enviando o modelo para o repositorio Hugging Face."
      ],
      "metadata": {
        "id": "aP6b1ZEGqoHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub_gguf('llama-3-8b-bnb-4bit-perguntas-respostas-medicina', tokenizer, quantization_method = 'q4_k_m')"
      ],
      "metadata": {
        "id": "xKp2d-kWERtc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}